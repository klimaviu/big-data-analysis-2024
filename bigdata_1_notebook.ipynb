{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klimaviu/big-data-analysis-2024/blob/main/bigdata_1_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing parallelization techniques with sequential image processing\n",
        "\n",
        "Disclaimer: the code was generated by iteratively consulting ChatGPT and making adjustments."
      ],
      "metadata": {
        "id": "ASxabsyw7CbV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBQQniW16v7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f447bb6-203c-411a-a2f2-f401e78ea82b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
        "import time\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = '/content/drive/My Drive/data_set_VU_test1.zip'\n",
        "output_directory = '/content/drive/My Drive/processed_images'\n",
        "\n",
        "executor = ThreadPoolExecutor(max_workers=10)\n",
        "\n",
        "def save_image(image_data, filename):\n",
        "    cv2.imwrite(os.path.join(output_directory, filename), image_data)\n",
        "\n",
        "# Define individual processing functions\n",
        "def convert_to_bw(image_data, filename):\n",
        "    _, bw_image = cv2.threshold(image_data, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    save_image(bw_image, f\"{filename}_bw\")\n",
        "    return bw_image\n",
        "\n",
        "def apply_blur(image_data, filename):\n",
        "    blurred_image = cv2.GaussianBlur(image_data, (5, 5), 0)\n",
        "    save_image(blurred_image, f\"{filename}_blur\")\n",
        "    return blurred_image\n",
        "\n",
        "def add_noise(image_data, filename):\n",
        "    black_pixels = np.sum(image_data == 0)\n",
        "    num_noise_pixels = int(0.1 * black_pixels)\n",
        "    h, w = image_data.shape\n",
        "    noise_x = np.random.randint(0, w, num_noise_pixels)\n",
        "    noise_y = np.random.randint(0, h, num_noise_pixels)\n",
        "    noisy_image = np.copy(image_data)\n",
        "    noisy_image[noise_y, noise_x] = 255 - noisy_image[noise_y, noise_x]\n",
        "    save_image(noisy_image, f\"{filename}_noise\")\n",
        "    return noisy_image\n",
        "\n",
        "# Load images from ZIP file\n",
        "def load_images(zip_path):\n",
        "    images_data = []\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        jpg_files = [f for f in zip_ref.namelist() if f.endswith('.jpg') and 'Images/' in f]\n",
        "        for file_name in jpg_files:\n",
        "            with zip_ref.open(file_name) as image_file:\n",
        "                image_data = image_file.read()\n",
        "                image_array = np.frombuffer(image_data, np.uint8)\n",
        "                image = cv2.imdecode(image_array, cv2.IMREAD_GRAYSCALE)\n",
        "                images_data.append((image, os.path.basename(file_name)))\n",
        "    return images_data\n",
        "\n",
        "# Function to process a single image for all tasks\n",
        "def process_image_for_all_tasks(img_data):\n",
        "    img, filename = img_data\n",
        "    for task_func in [convert_to_bw, apply_blur, add_noise]:\n",
        "        img = task_func(img, filename)\n",
        "\n",
        "# Function to process all images sequentially\n",
        "def process_images_sequential(images_data):\n",
        "    start_time = time.time()\n",
        "    for img_data in images_data:\n",
        "        process_image_for_all_tasks(img_data)\n",
        "    return time.time() - start_time\n",
        "\n",
        "def process_all_images_for_all_tasks_parallel(images_data, b=50):\n",
        "    start_time = time.time()\n",
        "    batch_size = b\n",
        "    processed_images = []\n",
        "\n",
        "    # Split the images into batches\n",
        "    image_batches = [images_data[i:i + batch_size] for i in range(0, len(images_data), batch_size)]\n",
        "\n",
        "    # Process images in batches of size batch_size in parallel\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for batch in image_batches:\n",
        "            for task_func in [convert_to_bw, apply_blur, add_noise]:\n",
        "                futures.append(executor.submit(process_batch, batch, task_func))\n",
        "\n",
        "        # Wait for all futures to complete\n",
        "        for future in futures:\n",
        "            processed_images.extend(future.result())\n",
        "\n",
        "    return time.time() - start_time\n",
        "\n",
        "def process_batch(batch, task_func):\n",
        "    return [task_func(image_data, filename) for image_data, filename in batch]\n",
        "\n",
        "# Function to process images for a specific task in parallel using global executor and map\n",
        "def process_images_for_task_parallel(task_func, images_data):\n",
        "    start_time = time.time()\n",
        "    processed_images = list(executor.map(lambda x: task_func(x[0], x[1]), images_data))\n",
        "    return time.time() - start_time\n",
        "\n",
        "def process_images_for_all_tasks_parallel(images_data):\n",
        "    start_time = time.time()\n",
        "    # Create a list of tuples for each image and task function\n",
        "    image_task_pairs = [(image_data, task_func, filename) for image_data, filename in images_data for task_func in [convert_to_bw, apply_blur, add_noise]]\n",
        "    # Process all images for all tasks in parallel\n",
        "    list(executor.map(lambda x: x[1](x[0], x[2]), image_task_pairs))\n",
        "    return time.time() - start_time\n",
        "\n",
        "# Evaluate performance\n",
        "def evaluate_performance(zip_path):\n",
        "    images_data = load_images(zip_path)\n",
        "\n",
        "    # Sequential processing\n",
        "    time_sequential = process_images_sequential(images_data)\n",
        "\n",
        "    # Parallel processing (task-based)\n",
        "    time_tasks_parallel = process_images_for_all_tasks_parallel(images_data)\n",
        "\n",
        "    # Parallel processing (file-based)\n",
        "    time_files_parallel = process_all_images_for_all_tasks_parallel(images_data)\n",
        "\n",
        "    print(f\"Sequential: {time_sequential:.2f}s, Task-based Parallel: {time_tasks_parallel:.2f}s, File-based Parallel: {time_files_parallel:.2f}s\")\n",
        "\n",
        "evaluate_performance(zip_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4snrMHtyi3T"
      },
      "source": [
        "## Selecting the optimal batch size for file-based parallelization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ogEYMXyy_Lo"
      },
      "outputs": [],
      "source": [
        "def cross_validate_batch_size(images_data, batch_sizes, n_fold=5):\n",
        "\n",
        "    performance_metrics = []\n",
        "    for b in batch_sizes:\n",
        "        total_time = 0\n",
        "        for _ in range(n_fold):\n",
        "            start_time = time.time()\n",
        "            process_all_images_for_all_tasks_parallel(images_data, b)\n",
        "            total_time += time.time() - start_time\n",
        "        avg_time = total_time / 3\n",
        "        performance_metrics.append(avg_time)\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "images_data = load_images(zip_path)\n",
        "batch_sizes = [1, 5, 10, 50, 75, 100, 125, 150]\n",
        "\n",
        "time_based_on_batch_size = cross_validate_batch_size(images_data, batch_sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlWbjf2z0n6N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn.objects as so\n",
        "\n",
        "time_vs_batch_size = pd.DataFrame({\n",
        "    \"batch_size\": batch_sizes,\n",
        "    \"avg_time\": time_based_on_batch_size\n",
        "})\n",
        "\n",
        "time_vs_batch_size\n",
        "\n",
        "so.Plot(time_vs_batch_size, \"batch_size\", \"avg_time\")\\\n",
        "  .add(so.Line(marker=\"o\", edgecolor=\"w\"), linestyle=None)\\\n",
        "  .label(\n",
        "      title = \"Batch size vs. average time\"\n",
        "  )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}